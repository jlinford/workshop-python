<HTML>
<HEAD>
<TITLE>The ASCI SWEEP3D README file</TITLE>
</HEAD>
<BODY>
<IMG SRC="http://www.llnl.gov/asci_benchmarks/asci.gif">
<H1>The ASCI SWEEP3D README File</H1>
<HR>
<H2>Table of contents</H2>

<UL>
<LI><A HREF="#Code Description">Code Description</A>
<LI><A HREF="#Building">Building Issues</A>
<LI><A HREF="#Files">Files in this distribution</A>
<LI><A HREF="#Optimization">Optimization Constraints</A>
<LI><A HREF="#Execution">Execution Issues</A>
<LI><A HREF="#Timing">Timing Issues</A>
<LI><A HREF="#Storage">Storage Issues</A>
<LI><A HREF="#About">About the data</A>
<LI><A HREF="#Expected">Expected Results</A>
<LI><A HREF="#Modification">Modification Record</A>
<LI><A HREF="#Record">Record of Formal Questions and Answers</A>
</UL>

<A NAME="Code Description">
<H2>Code Description</H2>
</A>
<P>

SWEEP3D: 3D Discrete Ordinates Neutron Transport
<P>

A.  General Description
<P>

The benchmark code SWEEP3D represents the heart of a real ASCI
application.  It solves a 1-group time-independent discrete ordinates
(Sn) 3D cartesian (XYZ) geometry neutron transport problem.  The XYZ
geometry is represented by an IJK logically rectangular grid of cells.
The angular dependence is handled by discrete angles with a spherical
harmonics treatment for the scattering source.  The solution involves
two steps:  the streaming operator is solved by sweeps for each angle
and the scattering operator is solved iteratively.
<P>

The two step solution coded in SWEEP3D is known as an inner iteration.
A realistic Sn code would solve a multi-group problem, which in simple
terms is nothing more than a group-ordered iterative solution on top of
what SWEEP3D does.  Groups are solved sequentially since there is
strong coupling between groups due to downscattering.  The multi-group
iterations are known as outer iterations.  Finally, a realistic ASCI Sn
code would include time-dependence with 1000's of time steps on top of
what a multi-group code does.  Work and memory increases substantially
for these real problems.  The increased work is predominantly just more
inner iterations.  Multi-group problems add another dimension to the
flux moments array while time-dependent problems are even worse since
they require saving the angle-dependent phi array for all grid points,
all angles, and all energy groups at both the old and new times.
<P>

An Sn sweep proceeds as follows.  For a given angle, each grid cell has
4 equations with 7 unknowns (6 faces plus 1 central); boundary
conditions complete the system of equations.  The solution is by a
direct ordered solve known as a sweep.  Three known inflows allow the
cell center and 3 outflows to be solved.  Each cell's solution then
provides inflows to 3 adjoining cells (1 each in the I, J, & K
directions).  This represents a wavefront evaluation with a recursion
dependence in all 3 grid directions.  For XYZ geometries, each octant
of angles has a different sweep direction through the mesh, but all
angles in a given octant sweep the same way.
<P>

This version of SWEEP3D is based on blocks of IJK & angles with a
stride-1 line-recursion in the I-direction as the innermost work unit.
The stride-1 I-line recursion is good for microprocessors and the
blocked domains provide parallelism opportunities.
<P>

B.  Coding
<P>

This version of SWEEP3D is written entirely in Fortran77 except that it
requires automatic arrays and a C timer routine is used.
All automatic arrays are in inner_auto().
The Sn solution is carried out by inner() and sweep():  the source
iterations are handled by inner() and the sweeps are handled in
sweep().  This solution process is all that is timed.
<P>

Explicit parallelism is by domain decomposition and message-passing.
This version of SWEEP3D supports both PVM and MPI message passing
libraries as well as a single processor version.  All message library
dependencies are isolated to the msg_stuff.cpp file which is
preprocessed to activate the appropriate code.  Parallelism by
multitasking the loops in inner() and sweep() is also possible.  Both
forms of parallelism can be exploited at the same time.
<P>

C.  Parallelization
<P>

The only inherent parallelism is over the discrete angles:  each sweep
for a given angle is independent.  But possible reflective boundary
conditions restrict this form of parallelism to a single octant of
angles.  Typical Sn orders of S4, S6, or S8 would provide only 3, 6, or
10-way parallelism so this doesn't go very far.  Even for the case of
all vacuum BCs, there would only be 24, 48, or 80-way parallelism.
This also doesn't work that well across distributed memory systems such
as MPPs or clusters of machines as it requires multiple copies of the
full spatial grid.
<P>

SWEEP3D exploits parallelism via a wavefront process.  First, a 2D
spatial domain decomposition onto a 2D array of processors in the I-
and J-directions is used.  A single wavefront solve on these domains
provides limited parallelism.  So to improve parallel efficiency,
blocks of work are pipelined through the domains.  SWEEP3D is coded to
pipeline blocks of MK k-planes and MMI angles through this 2D processor
array.  Through a specific ordering of octants, an upper and lower
octant pair also gets pipelined.  And lastly, the sweeps of the next
octant pair start before the previous wavefront is completed; the
octant order required for reflective BCs limits this overlap to two
octant pairs at a time.  The overall combination is sufficient to give
good theoretical parallel utilization.  The resulting pipelined
wavefronts are depicted below just as the second wavefront gets started
in the lower-right corner and heads toward the upper-left corner (the
first wavefront is still finishing up its traversal from upper-right to
lower-left).
<P>

<PRE>
                 Parallelism via Domain Decomposition
                           IJ (top) View

                           NPE_I domains
   -----------------------------------------------------------------
   |               |               |               |               |
   |               |               |               |               |
   |               |               |               |               |
   |               |               |               |               |
   |               |               |               |               |
   -----------------------------------------------------------------
N  |               |               |               |               |
P  |  octant 2     |               |               |               |
E  |  m-block MMO  |               |               |               |
|  |  k-block KB   |               |               |               |
J  |               |               |               |               |
   -----------------------------------------------------------------
d  |               |               |               |               |
o  |  octant 2     |  octant 2     |               |               |
m  |  m-block MMO  |  m-block MMO  |               |               |
a  |  k-block KB-1 |  k-block KB   |               |               |
i  |               |               |               |               |
n  -----------------------------------------------------------------
s  |               |               |               |               |
   |  octant 2     |  octant 2     |  octant 2     |  octant 3     |
   |  m-block MMO  |  m-block MMO  |  m-block MMO  |  m-block 1    |
   |  k-block KB-2 |  k-block KB-1 |  k-block KB   |  k-block 1    |
   |               |               |               |               |
   -----------------------------------------------------------------
</PRE>

The theoretical parallel processor utilization based on this 2D
decomposition with pipelining (assuming MK is an integral factor of KT
with KB=KT/MK and MMI is an integral factor of MM with MMO=MM/MMI) is
given by
<P>
<PRE>
                        8*MMO*KB * (NPE_I*NPE_J)
 -----------------------------------------------------------------------
  2*[2*MMO*KB+(NPE_J-1) + 2*MMO*KB+(NPE_I-1)+(NPE_J-1)] * (NPE_I*NPE_J)
</PRE>
<P>

Multitasking parallelism can also be exploited within each block of
work but requires a special ordering of that work.  A JK-diagonal
wavefront yields independent I-line work units at each (J,K), thus
enabling multitasking work sharing.  The amount of parallelism builds
up and falls off based on the length of the JK-diagonal, so parallel
processor utilization isn't perfect.  Also, synchronization is required
after each diagonal.  To help improve the situation, MMI angles are
pipelined on JK-diagonals to increase the amount of I-lines that can be
solved in parallel.  This MMI-pipelined JK-diagonal wavefront is
depicted below at the 4th stage of a 3-deep wavefront that started in
the upper right corner.
<P>

<PRE>
          Parallelism via Diagonal Wavefront Multitasking
                         JK (side) View
 
                          JT grid points
     ---------------------------------------------------------
     |       |       |       |       |       |       |       |
     |       |       |       |  mi=1 |  mi=2 |  mi=3 |       |
     |       |       |       |       |       |       |       |
 M   ---------------------------------------------------------
 K   |       |       |       |       |       |       |       |
     |       |       |       |       |  mi=1 |  mi=2 |  mi=3 |
 g   |       |       |       |       |       |       |       |
 r   ---------------------------------------------------------
 i   |       |       |       |       |       |       |       |
 d   |       |       |       |       |       |  mi=1 |  mi=2 |
     |       |       |       |       |       |       |       |
 p   ---------------------------------------------------------
 o   |       |       |       |       |       |       |       |
 i   |       |       |       |       |       |       |  mi=1 |
 n   |       |       |       |       |       |       |       |
 t   ---------------------------------------------------------
 s   |       |       |       |       |       |       |       |
     |       |       |       |       |       |       |       |
     |       |       |       |       |       |       |       |
     ---------------------------------------------------------
</PRE>

The theoretical processor utilization for the MMI-pipelined diagonal
wavefront on NCPU processors (assuming MK is an integral factor of KT,
MMI is an integral factor of MM, and NPE_J is an integral factor of
JT_G with JT=JT_G/NPE_J) is given by
<P>
<PRE>
                               MMI*JT*MK
----------------------------------------------------------------------
SUM [
  (SUM [max(min(i-mi+1,JT,MK,JT+MK-i+mi-1),0);mi=1,mmi] + NCPU-1)/NCPU
    ;i=1,JT+MK-1+MMI-1 ] * NCPU
</PRE>
<P>

Multitasking the rest of sweep() is also possible.  Multitasking on the
I, J, or K loops in loop nests is likely better than on the MI loop
because MMI is so small.  Also, both face and leakage are reductions:
face on angles and leakage on angles and spatial boundary faces.  The
MMI-pipelined JK-diagonals are such that these reductions are not a
problem inside of the main diagonal loop.  But care must be taken when
multitasking other loops involving these reductions.  Fusing loops
where possible might help with multitasking efficiency.  Multitasking
the source() and flux_err() routines should be easy; they can even be
rewritten in a single long 1D IJK-loop form.  This form of multitasking
has been proven possible on a CRI YMP using their autotasking support.
<P>

As one might expect, there are several factors and tradeoffs in
parallel efficiencies.  The most important is that the explicit domain
decomposition and multitasking efficiencies are inversely correlated in
MK and MMI.  The best domain decomposition efficiency is when MK=1 and
MMI=1 and the best multitasking efficiency is when MK=KT and MMI=MM.
Another tradeoff is that adding another row or column of processors
will reduce efficiency in both cases.  Non-integral blocking of
k-planes can lead to load imbalances.  Message passing latencies are
best hidden when fewer larger messages are used, but this implies
larger blocks via larger MK & MMI, which in turn decreases the explicit
parallel efficiency.  Multiprocessor synchronization overhead is best
hidden with larger blocks (ie. longer diagonals) as well.  The size of
a block can also have cache effects.  Selection of "best" values for MK
and MMI are likely system dependent, where the system in this case is
the cluster of N-way SMPs.
<P>

Asynchronous (or non-blocking) send/receives can also be used.  A
simple scheme would simply overlap the pair of sends and the pair of
receives.  One could also overlap communication for the next block with
the work on the current block.  But this would require double buffering
the Phiib, Phiij, & Phikb arrays.  It is not clear how much this will
help since the neighbor processors are likely to be being computing the
needed messages at the same time the current processor is busy anyway.
Communication can also be overlapped with the source, flux, and DSA
current computations within a block by pulling them out of the diagonal
loop and computing them either before the receives or after the sends.
But this would require promoting several arrays from I-line temporaries
to full block size and adding separate loop nests, and this is likely
to slow things down due to the increased memory traffic.
<P>

<A NAME="Building">
<H2>Building Issues</H2>
</A>
<P>

Type "make" at a shell prompt and follow the instructions.  The vendor
is expected to provide the appropriate system calls in timers.f to
return local processor CPU time and global elapsed (wall clock) time.
The vendor is expected to modify the compiler related macros and PVM
and MPI macros in the makefile.
<P>

Three versions of the code can be built: single processor, PVM, & MPI.
This is all handled in msg_stuff.cpp via the CPP C-preprocessor and
ifdef's.  The executables are named sweep3d.single, sweep3d.pvm, &
sweep3d.mpi.  The PVM version requires external setup of the PVM
configuration and uses a master/slave spawning model with the
appropriate number of slaves spawned with one call to pvmfspawn() using
the default round-robin scheme.  The PVM version also has
psend()/precv() and reductions, but they have not been tested.  The MPI
version runs under mpirun and uses the rank=0 member as the master.
<P>

<A NAME="Files">
<H2>Files in this distribution</H2>
</A>
<PRE>
      README.html             README in HTML format
      README                  README in ASCII text
      driver.f                main
      inner_auto.f            automatic arrays
      inner.f                 inner iteration loop
      sweep.f                 sweeper
      source.f                source moments
      flux_err.f              convergence test
      read_input.f            input
      decomp.f                domain decomposition
      initialize.f            problem setup
      octant.f                sweep directions
      timers.c                timers (Fortran callable)
      msg_stuff.cpp           message passing support library
      msg_stuff.h             message passing support
      makefile                make file
      input*                  input files
      output*                 sample output files
</PRE>

<A NAME="Optimization">
<H2>Optimization Constraints</H2>
</A>
<P>

The entire code can be compiled with either F77 or F90 compilers, but
the selected compiler and its options must be the same for all source
files except inner_auto.f as explained below.
<P>

This version of the SWEEP3D code requires automatic arrays which is a
typical extension of Fortran77 and a standard feature of Fortran90.
All automatics are isolated to the file inner_auto.f.  Vendors must
retain these automatic arrays, but may compile inner_auto.f differently
from the rest of the code (eg. different compiler options or F90 for
just this file).
<P>

The cross sections, external source, and geometry arrays (Sigt, Sigs,
Srcx, hi, hj, hk, di, dj, & dk) are set to constants in the code to
provide a simple problem setup.  In real practice, this won't be the
case, so optimizations that "demote" these arrays to constants or
scalars will not be allowed.  Also, the code options ISCT, IBC, JBC,
KBC, & IDSA must remain viable; optimizing the code for the specific
settings will not be allowed.
<P>

The MPI message passing version must be used for cases involving domain
decomposition.  The PVM version is simply an alternative for testing.
<P>

Vendors are permitted to make changes to the supplied  Fortran codes to
optimize for data layout and alignment.  Wholesale changes of the
parallel algorithms are also permitted as long as the full capabilities
of the code are maintained and a full description is provided.
<P>

The constraints for audit trails following code modification as noted
in the RFP apply.
<P>

<A NAME="Execution">
<H2>Execution Issues</H2>
</A>
<P>

Benchmark code SWEEP3D must be run using 64-bit floating-point
precision for all non-integer data variables.
<P>
Virtually all of the time is spent in sweep() performing the sweeps for
all of the angles.
<P>

<A NAME="Timing">
<H2>Timing Issues</H2>
</A>
<P>

All timings made by this code are via the timers() subroutine in
timers.f.  The source should be modified by the vendor to provide both
CPU and wall (elapsed) times for the particular hardware with
acceptable accuracy and resolution.
<P>

Only the Sn solution process is timed via two timing calls in inner.f.
Only the master process does the timing, but all processes are
barriered before each timing call so as to measure the collective time
of all processors.  The solution time is also expressed in terms of a
"grind" time, which is the effective time to  "update" a spatial &
angle phase-space cell.  The T3D PVM version can run the required
150-cubed problem on 128 PEs with an elapsed grind time of 0.037
microseconds.  The best elapsed grind times we have ever seen are in
the 0.008 microsecond range for a 256-cubed version of the same problem
on 512 PEs of a T3D on a "cousin" version of SWEEP3D which uses SHMEM
communications.
<P>

<A NAME="Storage">
<H2>Storage Issues</H2>
</A>
<P>

Almost all of the storage requirements comes from the automatic arrays
in inner_auto().  The printout gives an estimate of memory usage in
MB/domain based on these arrays and the decomposition and blocking.
<P>

The small 50-cubed problem only requires 16MB total.  The 150-cubed
problem requires 434MB total.  These are small because this is only a
1-group time-independent problem.  A 20 group problem would be 2.5GB,
and a 20-group time-dependent problem would be 54GB and would run for a
week or so even at a 0.010 microsecond grind time.
<P>

The minumum memory configuration required to run the problems in each
configuration must be reported (OS + buffers + code + data + ...).
<P>

<A NAME="About">
<H2>About the data</H2>
</A>
<P>

Two inputs are to be run and their outputs reported: input.50 and
input.150.  These two input files correspond to 50-cubed and 150-cubed
spatial grid point problems.  Each is expected to be run and the
results reported on
<P>
<OL>
<LI> a single processor
<LI> on all processors of a single SMP
<LI> on all processors in a cluster of SMPs comprising
     a 30 PEAK GFLOPS system
</OL>
<P>

Case 2 is expected to use just domain decomposition & MPI within an
SMP, just multitasking, or both simultaneously.  Case 3 is expected to
use just domain decomposition & MPI both within and across SMPs or
domain decomposition & MPI across SMPs and multitasking within SMPs.
The blocking factors MK and MMI can be chosen as desired.  Alternative
parallel implementations are also acceptable as long as they rely on
MPI and/or multitasking.
<P>

The elapsed wall clock time and minimum memory configuration required to
run the 150-cubed problem will be scored.
<P>

The input file must be named "input" and be in the current working
directory.  A problem description is printed which reflects the input
parameters.  It also gives memory estimates, global messaging counts,
and theoretical domain decomposition and multitasking parallel
efficiencies based on the pipelined domain and diagonal JK-wavefronts
described above.  The multitasking efficiency is based on NCPU read in
from the input file.
<P>

Only the first line of these files should be changed by the vendor.
The first line of input controls both the explicit domain decomposition
and multitasking parallelism.  The blocking factors MK and MMI can be
chosen as desired.  The NCPUS value is only used in calculating the
estimated theoretical multitasking efficiency.  The rest of the input
file controls problem setup and code options and is not to be changed.
<P>

The format of the "input" file is as follows:
<P>

<PRE>
Line #  Variables   Type        Explanation and Value(s)

  1       NPE_I     integer     # PEs in the I-direction
          NPE_J     integer     # PEs in the J-direction
          MK        integer     # K-planes for blocking
          MMI       integer     # angles for blocking (must factor MM)
          NCPU      integer     # CPUs per SMP to calc multitasking effic

  2       IT_G      integer     # grid points in the I-direction
          JT_G      integer     # grid points in the J-direction
          KT        integer     # grid points in the K-direction
          MM        integer     # angles per octant (3 or 6)
          ISCT      integer     Pn scattering order (0 or 1)

  3       DX        real        delta-x for I-direction
          DY        real        delta-y for J-direction
          DZ        real        delta-z for K-direction
          EPSI      real        convergence control
                                  < 0.0   =>  iteration count
                                  > 0.0   =>  epsilon criterion

  4       IBC       integer     BC flag for I-direction
          JBC       integer     BC flag for J-direction
          KBC       integer     BC flag for K-direction
                                   0 = vacuum
                                   1 = reflective

  5       IPRINT    integer     flux print flag
                                   0 = off
                                   1 = on
          IDSA      integer     DSA face-currents
                                   0 = off
                                   1 = on
          IFIXUPS   integer     flux fixup flag
                                   0 = off
                                   1 = on
                                 < 0 = on after -IFIXUPS iterations
</PRE>

<A NAME="Expected">
<H2>Expected Results</H2>
</A>
<P>

Printed results for the two problem sizes are provided in the
output.50* and output.150* files.  Vendor's results should match those
provided in these files to at least 10 significant digits for
floating-point quantities (iteration error and balance quantities).
Integer results should agree exactly.  As a physical consistency check,
absorption plus leakages should balance the external source (accuracy
improves with the number of iterations).
<P>

<A NAME="Modification">
<H2>Modification Record</H2>
</A>
<P>

<DL>
<DT> Version 1.0 (9/06/95)
<DD>
Original version of code placed on this WWW site.
<P>
<DT> Version 2.0 (9/11/95)
<DD>
The MMI-folded JK-diagonals were replaced by MMI-pipelined
JK-diagonals so that the leakage and face reductions in the diagonal
loop don't have to be guarded - this is a major improvement for
multitasking.  The multitasking efficiency calculation was also
added at this time.  A bug fix was made for non-integral k-plane
(MK) blocking.  Outputs were regenerated but results were consistent.
<P>
<DT> Version 2.1 (9/13/95)
<DD>
Only changed driver.f, sweep.f, & README, and added README.html.
Added a version printout to driver.f.  Changed sweep.f to ease
multitasking, improve efficiency by merging a few IF-blocks, and
rewrote several K-loops into LK-loop forms.  Reduction temporaries
LEAK and JFIXED were added to aid with multitasking guarded
updates.  This version was proven to autotask on a CRI YMP by simply
adding multitasking directives and reordering several loop nests.
The README was cleaned up and converted to HTML.  The plaintext
README is directly produced from the HTML version now.
<P>
<DT> Version 2.2 (10/20/95)
<DD>
Eliminated unnecessary use of phij/phik I-line temps in sweep() by
using phijb/phikb arrays directly in order to reduce memory
traffic.  Moved divides in sweep() balance equation loops towards
top of loops to hide divide pipeline latency as much as possible for
compilers that don't see this optimization.  Added support for CRI
T3D using PVM - some of which effects all flavors: 1) T3D spawning
model with PE# in place of TIDS; 2) encoding, integer type, and
group are now common variables instead of constants; 3) barrier()
was renamed barrier_sync().  Fixed PVM psend/precv alternative so it
actually works now.  Changed PVM/MPI include file logic in
msg_stuff.cpp to use #ifdefs for better portability.  Changed the
README to: 1) document that NCPU is read from the input; 2) that a
non-integral K-blocking (MK) bug was fixed in Version 2.1; 3) to
clarify the memory reporting requirement; 4) to change the clustered
SMP test configuration (case 3) to 1/4 of an initial ASCI 200 GFLOPS
configuration; 5) to include a 150-cubed timing for a T3D with this
exact version of the code.
Files changed: README, README.html, inner.f, inner_auto.f,
msg_stuff.cpp, msg_stuff.h, sweep.f
<P>
<DT> Version 2.2b (12/20/95)
<DD>
Minor changes.  The README.html now makes use of HTML markups, changed
the requirement for the clustered SMP run to a 30 PEAK GFLOPS system
configuration, and added some Questions and Answers.  The text README
file was regenerated.  All output files have been regenerated including
a new output.150.single from a completed 150-cubed single processor
run.  The timers routine was replaced by the more portable C code
version from SMPT; this required a few changes to the makefile.  The
driver.f file was changed only to update the version stamp to read "2.2b".
Files changed: README, README.html, makefile, timers.c, driver.f, output*.
<P>
</DL>

<A NAME="Record">
<H2>Record of Formal Questions and Answers</H2>
</A>

<DL>
<DT> Question 1:
<DD>
We noticed that the output.150.single has a segmentation fault.  
Can you provide a valid output.150.single?  Is there a problem with the 
150 case?
<DT> Answer 1:
<DD>
The output.150.single case faulted only because it exceeded stack space
limits during the automatic array allocations in inner_auto().
The code runs properly at 150-cubed on a single processor.  A new
output.150.single has been provided.
<DT> Question 2:
<DD>
On the SWEEP3D benchmark in general and the sweep routine in
particular, is the use of "special casing" in
the inner I-loop allowed ( "do i=i0,i1,i2" ) ?
<DT> Answer 2:
<DD>
Assuming this means explicitly coding the two possibilities
( "do i=1,it,+1" ) and ( "do i=it,1,-1" ), the answer is no.  It
requires the 2 balance equation loop bodies to be duplicated which is
unacceptable from a software enginnering perspective.
On Cray vector machines it is useful to use a 1st-order linear
recursion library routine for the recursion portion of these loops but
fixups are done after the fact and require restarting the recursion
from the fixed up point on.
<DT> Question 3:
<DD>
On the SWEEP3D benchmark in general and the sweep routine in particular         
is the "fix-up logic" on a coarser program granularity allowed?  
<DT> Answer 3:
<DD>
If this means moving the fixup IF-test outside of the IDIAG or JKM loops
or even two versions of the sweep subroutine, the answer is no.  Too
much code ends up being duplicated which is unacceptable from a software
enginnering perspective.
<DD>
</DL>

<HR>
</BODY>
</HTML>
